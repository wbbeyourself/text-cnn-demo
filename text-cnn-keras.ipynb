{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = os.path.abspath(os.path.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXN = 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imdb dataset url: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filetype):\n",
    "    \"\"\"\n",
    "    filetype: 'train' or 'test'\n",
    "    \"\"\"\n",
    "    all_labels = [1] * MAXN + [0] * MAXN\n",
    "    \n",
    "    pk_file = '%simdb_%s.pk' % (BASE, filetype)\n",
    "    if os.path.exists(pk_file):\n",
    "        all_texts = pickle.load(open(pk_file, 'rb'))\n",
    "        return all_texts, all_labels\n",
    "    \n",
    "    \n",
    "    all_texts = []\n",
    "    file_list = []\n",
    "    path = BASE + '/aclImdb/'\n",
    "    \n",
    "    # read positive\n",
    "    pos_path = path + filetype + '/pos/'\n",
    "    i = 0\n",
    "    for f in os.listdir(pos_path):\n",
    "        file_list.append(pos_path + f)\n",
    "        i += 1\n",
    "        if i == MAXN:\n",
    "            break\n",
    "    \n",
    "    # read negative\n",
    "    neg_path = path + filetype + '/neg/'\n",
    "    i = 0\n",
    "    for f in os.listdir(neg_path):\n",
    "        file_list.append(neg_path + f)\n",
    "        i += 1\n",
    "        if i == MAXN:\n",
    "            break\n",
    "    \n",
    "    for f in file_list:\n",
    "        with open(f) as f:\n",
    "            all_texts.append(rm_tags(' '.join(f.readlines())))\n",
    "    \n",
    "    # dump text\n",
    "    \n",
    "    with open(pk_file, 'wb') as f:\n",
    "            pickle.dump(all_texts, f)\n",
    "            \n",
    "    return all_texts, all_labels     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_texts, train_labels, test_texts, test_labels):\n",
    "    token = Tokenizer(num_words=2000)\n",
    "    token.fit_on_texts(train_texts)\n",
    "    x_train_seq = token.texts_to_sequences(train_texts)\n",
    "    x_test_seq = token.texts_to_sequences(test_texts)\n",
    "    x_train = sequence.pad_sequences(x_train_seq, maxlen=150, padding='post', truncating='post')\n",
    "    x_test = sequence.pad_sequences(x_test_seq, maxlen=150, padding='post', truncating='post')\n",
    "    y_train = np.array(train_labels)\n",
    "    y_test = np.array(test_labels)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn(maxlen=150, max_features=2000, embed_size=32):\n",
    "    # Inputs\n",
    "    comment_seq = Input(shape=[maxlen], name='x_seq')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment = Embedding(max_features, embed_size)(comment_seq)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "    filter_sizes = [2, 3, 4, 5]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.5)(merge)\n",
    "    output = Dense(32, activation='relu')(out)\n",
    "\n",
    "    output = Dense(units=1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model([comment_seq], output)\n",
    "    #     adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train num : 25000\n",
      "test num : 25000\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "22500/22500 [==============================] - 12s 551us/step - loss: 0.5904 - acc: 0.6633 - val_loss: 0.4395 - val_acc: 0.7716\n",
      "Epoch 2/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.3743 - acc: 0.8345 - val_loss: 0.4526 - val_acc: 0.7868\n",
      "Epoch 3/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.3144 - acc: 0.8656 - val_loss: 0.3492 - val_acc: 0.8432\n",
      "Epoch 4/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.2716 - acc: 0.8870 - val_loss: 0.4442 - val_acc: 0.8056\n",
      "Epoch 5/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.2384 - acc: 0.9038 - val_loss: 0.3608 - val_acc: 0.8452\n",
      "Epoch 6/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.2047 - acc: 0.9207 - val_loss: 0.4362 - val_acc: 0.8232\n",
      "Epoch 7/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.1730 - acc: 0.9341 - val_loss: 0.4929 - val_acc: 0.8136\n",
      "Epoch 8/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.1452 - acc: 0.9461 - val_loss: 0.3931 - val_acc: 0.8524\n",
      "Epoch 9/20\n",
      "22500/22500 [==============================] - 7s 317us/step - loss: 0.1220 - acc: 0.9561 - val_loss: 0.5183 - val_acc: 0.8256\n",
      "Epoch 10/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0964 - acc: 0.9664 - val_loss: 0.6142 - val_acc: 0.8096\n",
      "Epoch 11/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0801 - acc: 0.9716 - val_loss: 0.5838 - val_acc: 0.8296\n",
      "Epoch 12/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0701 - acc: 0.9769 - val_loss: 0.5961 - val_acc: 0.8364\n",
      "Epoch 13/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0580 - acc: 0.9804 - val_loss: 0.5791 - val_acc: 0.8512\n",
      "Epoch 14/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0501 - acc: 0.9820 - val_loss: 0.6671 - val_acc: 0.8392\n",
      "Epoch 15/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0484 - acc: 0.9837 - val_loss: 0.6247 - val_acc: 0.8488\n",
      "Epoch 16/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0420 - acc: 0.9862 - val_loss: 0.7515 - val_acc: 0.8324\n",
      "Epoch 17/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0387 - acc: 0.9870 - val_loss: 0.8180 - val_acc: 0.8240\n",
      "Epoch 18/20\n",
      "22500/22500 [==============================] - 7s 318us/step - loss: 0.0360 - acc: 0.9880 - val_loss: 0.9477 - val_acc: 0.8032\n",
      "Epoch 19/20\n",
      "22500/22500 [==============================] - 7s 319us/step - loss: 0.0361 - acc: 0.9869 - val_loss: 0.6786 - val_acc: 0.8588\n",
      "Epoch 20/20\n",
      "22500/22500 [==============================] - 7s 319us/step - loss: 0.0332 - acc: 0.9887 - val_loss: 0.9175 - val_acc: 0.8216\n",
      "accuracy: 0.8395, f1-score: 0.8419\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('./aclImdb'):\n",
    "        tfile = tarfile.open(r'./aclImdb_v1.tar.gz', 'r:gz')  # r;gz是读取gzip压缩文件\n",
    "        result = tfile.extractall('./')  # 解压缩文件到当前目录中\n",
    "    train_texts, train_labels = read_files('train')\n",
    "    test_texts, test_labels = read_files('test')\n",
    "    x_train, y_train, x_test, y_test = preprocessing(train_texts, train_labels, test_texts, test_labels)\n",
    "    \n",
    "    print('train num : %s' % len(x_train))\n",
    "    print('test num : %s' % len(x_test))\n",
    "    \n",
    "    model = text_cnn()\n",
    "    batch_size = 128\n",
    "    epochs = 20\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              validation_split=0.1,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              shuffle=True)\n",
    "    \n",
    "    predict = (np.asarray(model.predict(x_test))).round()\n",
    "    f1 = f1_score(y_test, predict)\n",
    "    acc = accuracy_score(y_test, predict)\n",
    "    model.save(BASE + '/word_textcnn_acc_%.4f_f1_%.4f.h5' % (acc, f1))\n",
    "    print('accuracy: %.4f, f1-score: %.4f' % (acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析哪些影评被预测错误，是否存在极性转移或者Aspect-level Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts, labels = read_files('test')\n",
    "\n",
    "# predict = pickle.load(open(BASE + '/predict.pk', 'rb'))\n",
    "\n",
    "# len(labels) == len(predict)\n",
    "\n",
    "# txt_labels = []\n",
    "# txts = []\n",
    "# for i in range(len(labels)):\n",
    "#     if labels[i] != predict[i]:\n",
    "#         txt_labels.append(labels[i])\n",
    "#         txts.append(texts[i])\n",
    "\n",
    "# len(txts) == 4013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将这些写入文件好好看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_list = []\n",
    "# for i in range(len(txts)):\n",
    "#     js = {}\n",
    "#     review = txts[i].replace('\\t', ' ').replace('\\r', ' ').replace('\\n', ' ')\n",
    "#     js['id'] = i\n",
    "#     js['label'] = txt_labels[i]\n",
    "#     js['content'] = review\n",
    "#     review_list.append(js)\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(review_list)\n",
    "# df = df[['label', 'content', 'id']]\n",
    "\n",
    "# df.to_csv(BASE + '/hard_classify_imdb_review_data.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
